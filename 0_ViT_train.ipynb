{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Subset\n",
    "import math\n",
    "from torch.functional import F\n",
    "import pennylane as qml\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "load_saved_model = False\n",
    "training_epoch = 150\n",
    "learning_rate = 0.0008\n",
    "momentum = 0.9\n",
    "weight_decay = 0.000005\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  1736 , Test size:  193\n"
     ]
    }
   ],
   "source": [
    "fraction_size = 0.3\n",
    "\n",
    "root_folder = \"AIDER\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 256x256\n",
    "    transforms.ToTensor(),          # Convert images to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats\n",
    "])\n",
    "dataset = datasets.ImageFolder(root=root_folder, transform=transform)\n",
    "\n",
    "# Split train and test and put in dataloader\n",
    "full_dataset_size = len(dataset)\n",
    "subset_size = int(fraction_size * full_dataset_size)\n",
    "subset_indices = torch.randperm(full_dataset_size)[:subset_size].tolist()\n",
    "subset = Subset(dataset, subset_indices)\n",
    "\n",
    "train_size = int(0.9 * len(subset))\n",
    "test_size = len(subset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(subset, [train_size, test_size])\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Train size: \", len(train_dataset), \", Test size: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUANTUM BLOCK\n",
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "n_layers = 4\n",
    "weight_shapes = {\"weights\": (n_layers, n_qubits)}\n",
    "dev_quantum = torch.device(device)\n",
    "\n",
    "# Define the quantum node\n",
    "@qml.qnode(dev)\n",
    "def qnode(inputs, weights):\n",
    "    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True, pad_with=0.0)\n",
    "    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "expanded_circuit = qml.transforms.broadcast_expand(qnode)\n",
    "class QNet(torch.nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.qlayer = qml.qnn.TorchLayer(expanded_circuit, weight_shapes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.qlayer(x.to('cpu')).to(dev_quantum)  # Ensure compatibility with quantum layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 5])\n"
     ]
    }
   ],
   "source": [
    "# from architectures.vit import ViT\n",
    "from vit_pytorch import ViT\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "#self.features = models.resnet34(pretrained =True)\n",
    "\n",
    "class QCNN(nn.Module): \n",
    "    def __init__(self) -> None:\n",
    "        super(QCNN, self).__init__()\n",
    "        # self.features = models.resnet34(pretrained =True)\n",
    "        self.vit = ViT(\n",
    "            image_size = 256,\n",
    "            patch_size = 32,\n",
    "            num_classes = 2,\n",
    "            dim = 1024,\n",
    "            depth = 10,\n",
    "            heads = 16,\n",
    "            mlp_dim = 2048,\n",
    "            dropout = 0.1,\n",
    "            emb_dropout = 0.1\n",
    "        )\n",
    "        self.qnet = QNet(4) # From 2 to 16\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 5)\n",
    "        ) # From 16 to 5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x)\n",
    "        x = self.qnet(x)\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 16,\n",
    "    num_classes = 5,\n",
    "    dim = 1024,\n",
    "    depth = 10,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "example = next(iter(dataloader_train))\n",
    "print(model(example[0].to(device)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Engine(object):\n",
    "    def __init__(self, model, optimizer, device, ema=None):\n",
    "        # Initialize the Engine with the model, optimizer, and the device it's running on.\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        # Current epoch of training.\n",
    "        self.cur_epoch = 0\n",
    "        # Number of iterations the training has run.\n",
    "        self.cur_iter = 0\n",
    "        # The best validation epoch, used to track the epoch with the best validation performance.\n",
    "        self.bestval_epoch = 0\n",
    "        # Lists to track the training and validation losses.\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        # Criterion for calculating loss. Here, it's Mean Squared Error Loss for regression tasks.\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    \"\"\" Block to begin training \"\"\"\n",
    "    def train(self, dataloader_train):\n",
    "        loss_epoch = 0.\n",
    "        num_batches = 0\n",
    "        # Set the model to training mode.\n",
    "        self.model.train()\n",
    "        \n",
    "        # Train loop\n",
    "        # tqdm is used to display the training progress for each epoch.\n",
    "        pbar = tqdm(dataloader_train, desc='Train Epoch {}'.format(self.cur_epoch))\n",
    "        for data in pbar:\n",
    "            # efficiently zero gradients\n",
    "            # Zero the gradients before running the backward pass.\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            images = data[0].to(self.device, dtype=torch.float32)   # Image that will be fed into network\n",
    "            gt_label = data[1].to(self.device, dtype=torch.long)\n",
    "\n",
    "            # Pass the images through the model to get predictions.\n",
    "            pred_label = self.model(images)\n",
    "            \n",
    "            # Calculate the loss, backpropagation, and optimization\n",
    "            loss = self.criterion(pred_label, gt_label)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Perform a single optimization step (parameter update).\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Aggregate the loss for the epoch\n",
    "            loss_epoch += float(loss.item())\n",
    "            num_batches += 1\n",
    "            pbar.set_description(\"Loss: {:.4f}\".format(loss.item()))\n",
    "            \n",
    "        pbar.close()\n",
    "        avg_loss = loss_epoch / num_batches\n",
    "        self.train_loss.append(avg_loss)\n",
    "        \n",
    "        self.cur_epoch += 1\n",
    "        pbar.set_description(\"Epoch: {}, Average Loss: {:.4f}\".format(self.cur_epoch, avg_loss))\n",
    "\n",
    "    def test(self, dataloader_test):\n",
    "        # self.model.eval()  # Set the model to evaluation mode\n",
    "        loss_epoch = 0.\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Prepare to collect predictions and ground truth\n",
    "        predictions = []\n",
    "        ground_truths = []\n",
    "        \n",
    "        with torch.no_grad():  # No need to calculate gradients\n",
    "            pbar = tqdm(dataloader_test, desc='Test Epoch {}'.format(self.cur_epoch))\n",
    "            for data in pbar:\n",
    "                images = data[0].to(self.device, dtype=torch.float32)   # Image that will be fed into network\n",
    "                gt_label = data[1].to(self.device, dtype=torch.long)  # GT_label\n",
    "\n",
    "                # Pass the images through the model to get predictions.\n",
    "                pred_label = self.model(images)\n",
    "\n",
    "                # Calculate the loss, backpropagation, and optimization\n",
    "                loss = self.criterion(pred_label, gt_label)\n",
    "                loss_epoch += float(loss.item())\n",
    "                num_batches += 1\n",
    "                \n",
    "                # We want to put this back on the CPU to calculate the metrics\n",
    "                predictions.extend(pred_label.argmax(dim=1).cpu().numpy().flatten())\n",
    "                ground_truths.extend(gt_label.cpu().numpy().flatten())\n",
    "                pbar.set_description(\"Test Loss: {:.4f}\".format(loss.item()))\n",
    "\n",
    "        avg_loss = loss_epoch / num_batches\n",
    "        self.val_loss.append(avg_loss)\n",
    "\n",
    "        # Print the accuracy here\n",
    "        accuracy = accuracy_score(ground_truths, predictions)\n",
    "        \n",
    "        print(f\"Test Epoch: {self.cur_epoch}, Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Total trainable parameters:  85030405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3416: 100%|██████████| 55/55 [00:38<00:00,  1.44it/s]\n",
      "Test Loss: 0.0575: 100%|██████████| 7/7 [00:02<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 1, Average Loss: 0.8695, Accuracy: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.9566: 100%|██████████| 55/55 [00:38<00:00,  1.42it/s]\n",
      "Loss: 0.7670: 100%|██████████| 55/55 [00:38<00:00,  1.43it/s]\n",
      "Loss: 0.9525: 100%|██████████| 55/55 [00:37<00:00,  1.45it/s]\n",
      "Loss: 0.5088: 100%|██████████| 55/55 [00:39<00:00,  1.39it/s]\n",
      "Loss: 0.6931: 100%|██████████| 55/55 [00:38<00:00,  1.43it/s]\n",
      "Loss: 1.5559: 100%|██████████| 55/55 [00:37<00:00,  1.47it/s]\n",
      "Loss: 0.6527: 100%|██████████| 55/55 [00:37<00:00,  1.45it/s]\n",
      "Loss: 0.5372: 100%|██████████| 55/55 [00:38<00:00,  1.43it/s]\n",
      "Loss: 0.8040: 100%|██████████| 55/55 [00:38<00:00,  1.41it/s]\n",
      "Loss: 1.6531: 100%|██████████| 55/55 [00:37<00:00,  1.45it/s]\n",
      "Test Loss: 0.0542: 100%|██████████| 7/7 [00:01<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 11, Average Loss: 0.7444, Accuracy: 0.7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.4593: 100%|██████████| 55/55 [00:38<00:00,  1.44it/s]\n",
      "Loss: 0.3901: 100%|██████████| 55/55 [00:38<00:00,  1.44it/s]\n",
      "Loss: 1.3136: 100%|██████████| 55/55 [00:38<00:00,  1.42it/s]\n",
      "Loss: 0.8036: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 0.5137: 100%|██████████| 55/55 [00:30<00:00,  1.80it/s]\n",
      "Loss: 0.8269: 100%|██████████| 55/55 [00:33<00:00,  1.65it/s]\n",
      "Loss: 1.0333: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.7129: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.9527: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.8012: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Test Loss: 0.1796: 100%|██████████| 7/7 [00:02<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 21, Average Loss: 0.8468, Accuracy: 0.6891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1747: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.9523: 100%|██████████| 55/55 [00:35<00:00,  1.57it/s]\n",
      "Loss: 0.5179: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.5682: 100%|██████████| 55/55 [00:33<00:00,  1.62it/s]\n",
      "Loss: 0.9054: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.2954: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.9948: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.0224: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.3930: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.9057: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Test Loss: 0.2432: 100%|██████████| 7/7 [00:01<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 31, Average Loss: 0.8057, Accuracy: 0.7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2872: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.3721: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.9986: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.1062: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.8672: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 0.6407: 100%|██████████| 55/55 [00:35<00:00,  1.57it/s]\n",
      "Loss: 0.4357: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.0458: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.1952: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.4250: 100%|██████████| 55/55 [00:36<00:00,  1.53it/s]\n",
      "Test Loss: 0.7898: 100%|██████████| 7/7 [00:01<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 41, Average Loss: 0.9868, Accuracy: 0.6269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8399: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.3606: 100%|██████████| 55/55 [00:33<00:00,  1.62it/s]\n",
      "Loss: 0.7480: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.8681: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.2427: 100%|██████████| 55/55 [00:36<00:00,  1.53it/s]\n",
      "Loss: 1.3053: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.3819: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.9123: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.6437: 100%|██████████| 55/55 [00:36<00:00,  1.52it/s]\n",
      "Loss: 1.4393: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Test Loss: 0.3726: 100%|██████████| 7/7 [00:01<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 51, Average Loss: 0.8319, Accuracy: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.0396: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.8518: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 1.6713: 100%|██████████| 55/55 [00:36<00:00,  1.52it/s]\n",
      "Loss: 1.3636: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.6824: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.9915: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.0868: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.9961: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.9784: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.0091: 100%|██████████| 55/55 [00:34<00:00,  1.59it/s]\n",
      "Test Loss: 0.3034: 100%|██████████| 7/7 [00:01<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 61, Average Loss: 0.8343, Accuracy: 0.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4285: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.7345: 100%|██████████| 55/55 [00:35<00:00,  1.57it/s]\n",
      "Loss: 0.7821: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.7528: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.5001: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.6701: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.6333: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.5438: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.2509: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.7251: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Test Loss: 0.1694: 100%|██████████| 7/7 [00:01<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 71, Average Loss: 0.7983, Accuracy: 0.7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2004: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.3786: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 0.8632: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.1991: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.8392: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.7492: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.7547: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 1.2887: 100%|██████████| 55/55 [00:34<00:00,  1.60it/s]\n",
      "Loss: 0.9913: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.3240: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Test Loss: 0.2946: 100%|██████████| 7/7 [00:01<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 81, Average Loss: 0.8124, Accuracy: 0.7306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8076: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.2763: 100%|██████████| 55/55 [00:36<00:00,  1.53it/s]\n",
      "Loss: 0.7955: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.9396: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.1826: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 1.1771: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.5826: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.9938: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 1.5540: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 1.3578: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Test Loss: 0.4781: 100%|██████████| 7/7 [00:01<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 91, Average Loss: 0.9161, Accuracy: 0.6943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8328: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 1.7198: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.9935: 100%|██████████| 55/55 [00:36<00:00,  1.52it/s]\n",
      "Loss: 1.1811: 100%|██████████| 55/55 [00:35<00:00,  1.57it/s]\n",
      "Loss: 0.6408: 100%|██████████| 55/55 [00:34<00:00,  1.58it/s]\n",
      "Loss: 0.7480: 100%|██████████| 55/55 [00:34<00:00,  1.57it/s]\n",
      "Loss: 0.2965: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.7953: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.6126: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.2220: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Test Loss: 0.3039: 100%|██████████| 7/7 [00:01<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 101, Average Loss: 0.8355, Accuracy: 0.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7207: 100%|██████████| 55/55 [00:36<00:00,  1.52it/s]\n",
      "Loss: 1.0744: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.3260: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 0.7780: 100%|██████████| 55/55 [00:35<00:00,  1.57it/s]\n",
      "Loss: 1.2782: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.6831: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.3906: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 1.4104: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.7414: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 1.0492: 100%|██████████| 55/55 [00:34<00:00,  1.57it/s]\n",
      "Test Loss: 0.2267: 100%|██████████| 7/7 [00:01<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 111, Average Loss: 0.8524, Accuracy: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8924: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.8318: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.2124: 100%|██████████| 55/55 [00:34<00:00,  1.62it/s]\n",
      "Loss: 0.8725: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.4923: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.1205: 100%|██████████| 55/55 [00:35<00:00,  1.57it/s]\n",
      "Loss: 1.0557: 100%|██████████| 55/55 [00:36<00:00,  1.52it/s]\n",
      "Loss: 0.8521: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.6856: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.6970: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Test Loss: 0.3981: 100%|██████████| 7/7 [00:01<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 121, Average Loss: 0.8707, Accuracy: 0.7047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5052: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.5503: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.7069: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 0.8083: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 1.2534: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.0558: 100%|██████████| 55/55 [00:35<00:00,  1.57it/s]\n",
      "Loss: 0.6509: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 1.0425: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.5612: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 1.1182: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Test Loss: 0.2376: 100%|██████████| 7/7 [00:01<00:00,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 131, Average Loss: 0.8488, Accuracy: 0.7202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8417: 100%|██████████| 55/55 [00:34<00:00,  1.61it/s]\n",
      "Loss: 0.9967: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.9317: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.2118: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 1.4052: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.1020: 100%|██████████| 55/55 [00:36<00:00,  1.52it/s]\n",
      "Loss: 0.3338: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 1.2415: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.1497: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.4609: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Test Loss: 0.3389: 100%|██████████| 7/7 [00:01<00:00,  3.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch: 141, Average Loss: 0.8958, Accuracy: 0.6943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8854: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.8666: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 2.0633: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 1.6772: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 1.3364: 100%|██████████| 55/55 [00:35<00:00,  1.55it/s]\n",
      "Loss: 1.1494: 100%|██████████| 55/55 [00:35<00:00,  1.53it/s]\n",
      "Loss: 0.8884: 100%|██████████| 55/55 [00:35<00:00,  1.54it/s]\n",
      "Loss: 0.3485: 100%|██████████| 55/55 [00:35<00:00,  1.56it/s]\n",
      "Loss: 0.6096: 100%|██████████| 55/55 [00:34<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = Engine(model, optimizer, device, ema=None)\n",
    "\n",
    "# Load the saved model if load_saved_model is set to True\n",
    "if load_saved_model:\n",
    "\tmodel.load_state_dict(torch.load('logs/final_model.pth'))\n",
    " \n",
    "# Count the total number of trainable parameters\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print ('======Total trainable parameters: ', params)\n",
    "\n",
    "for epoch in range(trainer.cur_epoch, training_epoch):\n",
    "\ttrainer.train(dataloader_train)\n",
    "\n",
    "\t# Test the model every 20 epochs and save it to logs folder\n",
    "\tif (epoch) % 10 == 0:\n",
    "\t\ttrainer.test(dataloader_test)\n",
    "\t\ttorch.save(model.state_dict(), os.path.join('logs', 'final_model.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
